{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Testing ML Feasibility with AutoGluon: A Hands-On Tutorial\n",
    "\n",
    "**MIDAS AI in Research Handbook — Chapter 12**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiaosuhu/midas-ai-in-research/blob/v1.0-dev/docs/notebooks/autogluon_tabular_demo.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks you through a complete ML feasibility test using AutoGluon on a tabular dataset. By the end, you will have trained a model, inspected which algorithms performed best, and identified which features matter most — all with under 20 lines of code.\n",
    "\n",
    "**Dataset:** A 500-row sample based on the California Housing dataset, where each row represents a census block group. The task is to predict the median house value (`MedHouseVal`) from neighborhood characteristics.\n",
    "\n",
    "**What you need:** A Google account to run this in Colab. No local installation required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Step 1 — Install AutoGluon\n",
    "\n",
    "This step only needs to run once per Colab session. It will take about 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogluon.tabular -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Step 2 — Load the Data\n",
    "\n",
    "We load the dataset directly from the GitHub repo, so there is nothing to upload or download manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_URL = \"https://raw.githubusercontent.com/xiaosuhu/midas-ai-in-research/v1.0-dev/docs/data/ca_housing_sample.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_URL)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-header",
   "metadata": {},
   "source": [
    "## Step 3 — Quick Look at the Data\n",
    "\n",
    "Before modeling anything, it is worth spending a minute understanding what we have. Each row is a census block group in California. The columns are:\n",
    "\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| `MedInc` | Median income (tens of thousands of USD) |\n",
    "| `HouseAge` | Median age of houses in the block |\n",
    "| `AveRooms` | Average number of rooms per household |\n",
    "| `AveBedrms` | Average number of bedrooms per household |\n",
    "| `Population` | Block group population |\n",
    "| `AveOccup` | Average number of occupants per household |\n",
    "| `Latitude` | Block group latitude |\n",
    "| `Longitude` | Block group longitude |\n",
    "| `MedHouseVal` | **Target** — Median house value (hundreds of thousands of USD) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "## Step 4 — Split into Train and Test Sets\n",
    "\n",
    "We hold out 20% of the data for final evaluation. AutoGluon handles its own internal validation during training, but this test set is reserved so we can assess performance on data the model has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training rows: {len(train_df)}\")\n",
    "print(f\"Test rows:     {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit-header",
   "metadata": {},
   "source": [
    "## Step 5 — Train with AutoGluon\n",
    "\n",
    "This is the core step. We tell AutoGluon which column is our target, give it a time budget, and let it run. It will train and evaluate multiple model types automatically — gradient boosting, random forests, neural networks, and others — then stack them into an ensemble.\n",
    "\n",
    "The `time_limit=120` means AutoGluon will stop after 2 minutes. For a feasibility test, that is usually plenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"MedHouseVal\",\n",
    "    eval_metric=\"rmse\",\n",
    "    path=\"autogluon_housing_model\"\n",
    ").fit(\n",
    "    train_data=train_df,\n",
    "    time_limit=120,\n",
    "    presets=\"medium_quality\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leaderboard-header",
   "metadata": {},
   "source": [
    "## Step 6 — Inspect the Leaderboard\n",
    "\n",
    "AutoGluon trains many models and ranks them by validation performance. The leaderboard is one of the most useful parts of the workflow — it shows you exactly what was tried and how each approach performed, rather than treating the process as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leaderboard-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = predictor.leaderboard(test_df, silent=True)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Step 7 — Evaluate on the Test Set\n",
    "\n",
    "Now we evaluate the best model on our held-out test set. Root Mean Squared Error (RMSE) tells us, on average, how far off our predictions are from the true house values. Since `MedHouseVal` is in hundreds of thousands of USD, an RMSE of 0.5 means we are off by about $50,000 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = predictor.evaluate(test_df)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance-header",
   "metadata": {},
   "source": [
    "## Step 8 — Feature Importance\n",
    "\n",
    "For research purposes, knowing which features drive predictions is often just as important as the prediction itself. AutoGluon estimates feature importance by measuring how much model performance drops when each feature is randomly shuffled — a method known as permutation importance.\n",
    "\n",
    "This can help you generate hypotheses, identify redundant variables, or flag potential data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "importance-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = predictor.feature_importance(test_df)\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "Let's visualize the feature importance to make it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importance_sorted = importance[\"importance\"].sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "importance_sorted.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "ax.set_xlabel(\"Permutation Importance\")\n",
    "ax.set_title(\"Feature Importance — AutoGluon Best Model\")\n",
    "ax.axvline(0, color=\"gray\", linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hands-on-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On Exercise\n",
    "\n",
    "Now it is your turn. Try modifying the code above to answer these questions:\n",
    "\n",
    "1. **Change the time limit** — what happens to the leaderboard if you give AutoGluon 5 minutes (`time_limit=300`) instead of 2?\n",
    "2. **Change the preset** — try `presets=\"best_quality\"` and compare RMSE to `medium_quality`. How much does it improve?\n",
    "3. **Drop a feature** — remove `Latitude` and `Longitude` from the training data using `train_df.drop(columns=[\"Latitude\", \"Longitude\"])`. Does performance drop? What does that tell you about these features?\n",
    "\n",
    "The cells below are yours to work in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Change the time limit\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Try a different preset\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Drop geographic features and retrain\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "This notebook covered the core tabular prediction workflow. AutoGluon also supports time series forecasting and multimodal data (combining text, images, and tables). See the [MIDAS AI in Research Handbook](https://midas-ai-in-research.readthedocs.io) for more, and the [AutoGluon documentation](https://auto.gluon.ai) for the full API reference.\n",
    "\n",
    "**Citation:** AutoGluon was developed by Amazon. If you use it in your research, please cite:\n",
    "\n",
    "> Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. *arXiv preprint arXiv:2003.06505*."
   ]
  }
 ]
}
